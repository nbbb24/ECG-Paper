# 📚 Research Papers Collection

A curated collection of research papers I'm reading, have read, or plan to read.

## 📑 Content by Topic

**Tools**

**Papers**
- [🧠 Reasoning](#-reasoning)
- [🤖 Agent](#-agent)
- [💬 Large Language Model](#-large-language-model)
- [📊 Physiological Signals](#-physiological-signals)
- [🔬 Multimodal](#-multimodal)

**Knowledge Base**
- [📖 AI/ML Knowledge](#-aiml-knowledge)

**Courses**
- [Courses](#courses)

---
## Tools
- **[ComfyAI](https://comfyai.app/)** - Collection of LLM techniques and workflows
- **[verl](https://github.com/volcengine/verl)** - Volcano Engine Reinforcement Learning for LLMs (RLHF framework supporting FSDP, vLLM, SGLang)
- **[FeatureDB](https://github.com/PKUDigitalHealth/FeatureDB)** - Pattern recognition methods for ECG feature extraction (expert features including HRV, morphologic variability, frequency domain, QRS axis)
- **[HeartPy](https://github.com/paulvangentcom/heartrate_analysis_python)** - Python Heart Rate Analysis Toolkit for PPG and ECG signals (time-domain & frequency-domain measures)
- **[Braindecode](https://github.com/braindecode/braindecode)** - Deep learning toolbox for decoding EEG, ECG, and MEG signals (PyTorch-based, includes datasets, preprocessing, models)


## Papers and Blogs

### 🧠 Reasoning
- **[Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)** - John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov (2017)
- **[Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)** - Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan (2023)
- **[Understanding the Math Behind GRPO — DeepSeek-R1-Zero](https://medium.com/yugen-ai-technology-blog/understanding-the-math-behind-grpo-deepseek-r1-zero-9fb15e103a0a)** - Soumanta Das, Yugen.ai (2025)
- **[DeepSeek-V3 Explained 1: Multi-head Latent Attention](https://medium.com/data-science/deepseek-v3-explained-1-multi-head-latent-attention-ed6bee2a67c4)** - Shirley Li (2025)
- **[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)** - Cameron R. Wolfe (2025)
- **[DeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training](https://medium.com/yugen-ai-technology-blog/deepseek-v3-advances-in-moe-load-balancing-and-multi-token-prediction-training-f6d68c59749c)** - Soumanta Das, Yugen.ai (2025)
- **[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)** - Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, et al. (2024)
- **[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)** - DeepSeek-AI (2025)
- **[QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training](https://arxiv.org/abs/2506.00711)** - Wei Dai, Peilin Chen, Chanakya Ekbote, Paul Pu Liang (2025)
- **[MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction](https://arxiv.org/abs/2509.23368)** - (2025)

### 🤖 Agent
- **[The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)** - A. Ali Heydari, Ken Gu, Vidya Srinivas, Hong Yu, et al. (2025)

### 💬 Large Language Model
- **[Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)** - Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean (2017)
- **[Deep contextualized word representations (ELMo)](https://arxiv.org/abs/1802.05365)** - Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer (2018)
- **[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)** - Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, et al. (2021)
- **[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)** - Meta AI (2024)
- **[Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115)** - Qwen Team, Alibaba (2024)

### 📊 Physiological Signals
- **[ENCASE: an ENsemble ClASsifiEr for ECG Classification Using Expert Features and Deep Neural Networks](https://www.cinc.org/archives/2017/pdf/178-245.pdf)** - Shenda Hong, Meng Wu, Yuxi Zhou, Qingyun Wang, Junyuan Shang, Hongyan Li, Junqing Xie (2017)
- **[ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram](https://arxiv.org/abs/2306.15681)** - Jungwoo Oh, Gyubok Lee, Seongsu Bae, Joon-myoung Kwon, Edward Choi (2023)
- **[Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data](https://arxiv.org/abs/2401.06866)** - Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, Hae Won Park (2024)
- **[A lightweight deep neural network for personalized detecting ventricular arrhythmias from a single-lead ECG device](https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0001037)** - Zhejun Sun, Wenrui Zhang, Yuxi Zhou, Shijia Geng, et al. (2025)
- **[ECG-Chat: A Large ECG-Language Model for Cardiac Disease Diagnosis](https://arxiv.org/abs/2408.08849)** - Yubao Zhao, Jiaju Kang, Tian Zhang, Puyu Han, Tong Chen (2024)
- **[ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling](https://arxiv.org/abs/2412.14373)** - William Han, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao (2024)
- **[GEM: Empowering MLLM for Grounded ECG Understanding with Time Series and Images](https://arxiv.org/abs/2503.06073)** - Xiang Lan, Feng Wu, Kai He, Qinghao Zhao, Shenda Hong, Mengling Feng (2025)
- **[Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework](https://arxiv.org/abs/2505.18847)** - William Han, Chaojing Duan, Zhepeng Cen, Yihang Yao, Xiaoyu Song, Atharva Mhaskar, Dylan Leong, Michael A. Rosenberg, Emerson Liu, Ding Zhao (2025)
- **[Retrieval-Augmented Generation for Electrocardiogram-Language Models](https://arxiv.org/abs/2510.00261)** - Xiaoyu Song, William Han, Tony Chen, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao (2025)
- **[SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)** - Yuwei Zhang, Kumar Ayush, Siyuan Qiao, A. Ali Heydari, et al. (2025)
- **[LSM-2: Learning from Incomplete Wearable Sensor Data](https://arxiv.org/abs/2506.05321)** - Maxwell A. Xu, Girish Narayanswamy, Kumar Ayush, Dimitris Spathis, et al. (2025)
- **[PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection](https://arxiv.org/abs/2509.19774)** - Xiaocheng Fang, Jiarui Jin, Haoyu Wang, Che Liu, Jieyi Cai, Guangkun Nie, Jun Li, Hongyan Li, Shenda Hong (2025)
- **[MEETI: A Multimodal ECG Dataset from MIMIC-IV-ECG with Signals, Images, Features and Interpretations](https://arxiv.org/abs/2507.15255)** - Deyun Zhang, Xiang Lan, Shijia Geng, Qinghao Zhao, Sumei Fan, Mengling Feng, Shenda Hong (2025)

### 🔬 Multimodal
- **[Zero-Shot Text-to-Image Generation (DALL-E)](https://arxiv.org/abs/2102.12092)** - Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever (2021)
- **[Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/abs/2103.00020)** - Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, et al. (2021)
- **[BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086)** - Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi (2022)
- **[Visual Instruction Tuning (LLaVA)](https://arxiv.org/abs/2304.08485)** - Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee (2023)
- **[CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models](https://arxiv.org/abs/2503.07667)** - Wei Dai, Peilin Chen, Malinda Lu, Daniel Li, Haowen Wei, Hejie Cui, Paul Pu Liang (2025)
- **[Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)** - Qwen Team, Alibaba (2025)

---

## 📖 AI/ML Knowledge

### 🤖 Reinforcement Learning from Human Feedback (RLHF)

**Reinforcement Learning from Human Feedback (RLHF)** aligns large language models (LLMs) with human preferences.  
It combines human evaluation, supervised fine-tuning (SFT), and reinforcement learning (RL) to produce helpful, safe, and aligned models.

#### 🧭 Overview

RLHF consists of three major stages:

1. **Supervised Fine-Tuning (SFT)** – Teach the model to follow instructions.
2. **Reward Model (RM)** – Learn to predict which responses humans prefer.
3. **Reinforcement Learning (RL)** – Optimize the model using feedback through PPO (Proximal Policy Optimization).

#### 🧩 Core Concepts

- **State space:** information available to the model  
- **Action space:** possible outputs (tokens)  
- **Reward function:** quantifies how good an answer is  
- **Policy (πθ):** the model's strategy for generating tokens  
- **Constraint (CLIP):** keeps updates stable  

#### 🧠 Step 1 — Pretraining

A transformer model is trained on massive text corpora to predict the next token, building language and reasoning abilities.

#### 📘 Step 2 — Supervised Fine-Tuning (SFT)

Fine-tune the model with instruction–response pairs from humans.

> Example  
> Prompt: "Summarize this article."  
> Response: "It discusses how RLHF aligns models with human feedback."

Mathematically:  
`y = πθ(x)`

#### ⭐ Step 3 — Reward Model Training

Humans rank responses (A1 > A2 > A3).  
The reward model outputs a scalar `rφ(x, y)` that reflects these preferences.

**Loss function:**  
`L = −log(σ(rφ(x, yc) − rφ(x, yr)))`

where:  
- `yc` = preferred answer  
- `yr` = rejected answer  
- `σ` = sigmoid function  

If the model ranks correctly → loss ≈ 0  
If it ranks incorrectly → loss increases.

#### 🎯 Step 4 — Reinforcement Learning (PPO)

**Models involved:**

| Model | Role |
|--------|------|
| **Actor (πθ)** | Generates responses |
| **Reward Model (rφ)** | Scores responses |
| **Critic (Vψ)** | Estimates expected reward |
| **Reference (π₀)** | Frozen SFT model for stability |

**Equations:**  
```
Reward:     r = rφ(x, y)
Value:      V = Vψ(x)
Advantage:  A = r − V
PPO Update: ratio = πθ(y|x)/π₀(y|x)
L_PPO = −E[min(ratio × A, clip(ratio, 1−ε, 1+ε) × A)]
```

#### 🧱 Simplified Pseudocode

```python
for step in training:
    y = actor.generate(x)
    reward = reward_model(x, y)
    value = critic(x)
    advantage = reward - value

    ratio = actor.prob(y|x) / ref_model.prob(y|x)
    clipped = clip(ratio, 1 - eps, 1 + eps)
    loss = -min(ratio * advantage, clipped * advantage)

    optimize(actor, loss)
    update(critic)
```

#### 🧱 ASCII Workflow Diagram

```
          ┌──────────────────────────────┐
          │        User Prompt x         │
          └──────────────┬───────────────┘
                         │
                         ▼
             ┌──────────────────────┐
             │   Actor Model πθ     │
             │ (Generates answer)   │
             └──────────┬───────────┘
                        │
                        ▼
             ┌──────────────────────┐
             │ Reward Model rφ(x,y) │
             │ (Scores answer)      │
             └──────────┬───────────┘
                        │
                        ▼
             ┌──────────────────────┐
             │   Critic Vψ(x)       │
             │ (Estimates value)    │
             └──────────┬───────────┘
                        │
                        ▼
             ┌──────────────────────┐
             │ Compute Advantage A  │
             │       A = r − V      │
             └──────────┬───────────┘
                        │
                        ▼
             ┌─────────────────────────────┐
             │ PPO Update + CLIP Constraint│
             │ Keep close to reference π₀  │
             └─────────────────────────────┘
```

#### ⚖️ Concept Summary

| Term | Meaning |
|------|----------|
| **Actor** | Learns to generate better answers |
| **Reward Model** | Judges human preference |
| **Critic** | Baseline expectation |
| **Advantage (A)** | How much better the action was than expected |
| **CLIP** | Prevents large, unstable updates |
| **Reference Model** | Keeps behavior near SFT baseline |

#### 🧩 Full Equation Summary

```
1. y = πθ(x)
2. r = rφ(x, y)
3. V = Vψ(x)
4. A = r − V
5. L_RM = −log(σ(r_c − r_r))
6. L_PPO = −E[min(ratio*A, clip(ratio,1−ε,1+ε)*A)]
```

#### 🧠 In Plain Words

1. Pretrain the model on text.  
2. Fine-tune it with human examples (SFT).  
3. Train a reward model to learn preferences.  
4. Use RL (PPO) to reinforce high-reward behaviors.  
5. Constrain updates with CLIP and a reference model for stability.

**Result:** A model that is *helpful, truthful, and aligned with human intent.*

#### 📚 References

- Christiano et al., "Deep Reinforcement Learning from Human Preferences," NeurIPS 2017  
- OpenAI (2022), "Training Language Models to Follow Instructions with Human Feedback"  
- Anthropic (2023), "Constitutional AI"  
- Hugging Face TRL — PPO for RLHF

---

## Courses

- **[Introduction to Deep Learning](https://deeplearning.cs.cmu.edu/F25/index.html)** - CMU 11-785, Fall 2025
- **[Large Language Models: Methods, Analysis, and Applications](https://cmu-llms.org/)** - CMU 11-667/11-867
- **[Advanced Natural Language Processing](https://cmu-l3.github.io/anlp-spring2025/)** - CMU 11-711, Spring 2025
- **[Multimodal Machine Learning](https://cmu-mmml.github.io/)** ([YouTube](https://www.youtube.com/@LPMorency)) - CMU 11-777
- **[How To AI (Almost) Anything](https://mit-mi.github.io/how2ai-course/spring2025/)** - MIT MAS.S60, Spring 2025
- **[Affective Computing and Multimodal Interaction](https://sites.google.com/media.mit.edu/2025acmmi/)** - MIT MAS.S63, Fall 2025

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
